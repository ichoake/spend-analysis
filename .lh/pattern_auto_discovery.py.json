{
    "sourceFile": "pattern_auto_discovery.py",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 5,
            "patches": [
                {
                    "date": 1749264806079,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1749264863098,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,15 +1,47 @@\n-import os, re, json\n+import os\n+import re\n+import json\n import pandas as pd\n from collections import Counter\n from dotenv import load_dotenv\n+import openai\n+import logging\n+from pathlib import Path\n+import sys\n \n-import openai\n-load_dotenv(os.path.expanduser(\"~/.env\"))\n-openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n+# --- Configuration ---\n GPT_CACHE_FILE = \"gpt_pattern_suggestions_auto.json\"\n+MERCHANT_CLUES_CSV = \"merchant_clues_auto.csv\"\n+TYPE_CLUES_CSV = \"type_clues_auto.csv\"\n+MERCHANT_GPT_CSV = \"merchant_clues_gpt_auto.csv\"\n+TYPE_GPT_CSV = \"type_clues_gpt_auto.csv\"\n+DEFAULT_TOP_N = 30\n \n+# --- Logging Setup ---\n+logging.basicConfig(\n+    level=logging.INFO,\n+    format=\"%(levelname)s: %(message)s\"\n+)\n+\n+def setup_openai():\n+    \"\"\"Load OpenAI API key from .env and set up openai module.\"\"\"\n+    env_path = os.path.expanduser(\"~/.env\")\n+    if not os.path.exists(env_path):\n+        logging.error(f\".env file not found at {env_path}. Please create it and add your OPENAI_API_KEY.\")\n+        sys.exit(1)\n+    load_dotenv(env_path)\n+    api_key = os.getenv(\"OPENAI_API_KEY\")\n+    if not api_key:\n+        logging.error(\"OPENAI_API_KEY not found in .env file.\")\n+        sys.exit(1)\n+    openai.api_key = api_key\n+\n def gpt_label_pattern(pattern_desc, cache):\n+    \"\"\"\n+    Query GPT to classify a merchant clue/type and generate a regex pattern.\n+    Uses a cache to avoid redundant API calls.\n+    \"\"\"\n     if pattern_desc in cache:\n         return cache[pattern_desc]\n     prompt = (\n         f\"This is a US bank transaction merchant clue or substring: '{pattern_desc}'. \"\n"
                },
                {
                    "date": 1749264882961,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -59,14 +59,83 @@\n             max_tokens=32,\n             temperature=0.1,\n         )\n         reply = resp['choices'][0]['message']['content'].strip()\n+        # Basic validation: ensure format is CATEGORY | REGEX\n+        if \"|\" not in reply:\n+            logging.warning(f\"Unexpected GPT reply format for '{pattern_desc}': {reply}\")\n+            reply = \"Other | .*\"\n     except Exception as e:\n-        print(f\"GPT ERROR: {e} for {pattern_desc[:30]}\")\n+        logging.error(f\"GPT ERROR: {e} for '{pattern_desc[:30]}'\")\n         reply = \"Other | .*\"\n     cache[pattern_desc] = reply\n     return reply\n \n+def extract_clues(df):\n+    \"\"\"\n+    Extract merchant and type clues from the Description column.\n+    Returns two Series: merchant clues and type clues.\n+    \"\"\"\n+    if 'Description' not in df.columns:\n+        raise ValueError(\"CSV must have a 'Description' column!\")\n+    merchant = df['Description'].str.extract(r'(?:CO:|NAME:)\\s*([A-Za-z0-9.\\- ]+)', expand=False)\n+    type_after = df['Description'].str.extract(r'TYPE:\\s*([A-Za-z0-9.\\- ]+)', expand=False)\n+    return merchant, type_after\n+\n+def save_counts(series, filename, top_n):\n+    \"\"\"Save value counts of a Series to CSV.\"\"\"\n+    counts = series.value_counts().dropna().head(top_n)\n+    counts.to_csv(filename)\n+    return counts\n+\n+def load_gpt_cache(cache_file):\n+    \"\"\"Load GPT cache from file, or return empty dict if not found.\"\"\"\n+    try:\n+        with open(cache_file, \"r\") as f:\n+            return json.load(f)\n+    except FileNotFoundError:\n+        return {}\n+\n+def save_gpt_cache(cache, cache_file):\n+    \"\"\"Save GPT cache to file.\"\"\"\n+    with open(cache_file, \"w\") as f:\n+        json.dump(cache, f, indent=2)\n+\n+def label_clues_with_gpt(clues_counts, clue_label, gpt_cache):\n+    \"\"\"\n+    For each clue, get GPT label/regex and return a list of dicts.\n+    clue_label: 'Merchant_Clue' or 'Type_Clue'\n+    \"\"\"\n+    results = []\n+    for clue in clues_counts.index:\n+        label = gpt_label_pattern(clue, gpt_cache)\n+        results.append({\n+            clue_label: clue,\n+            'Count': int(clues_counts[clue]),\n+            'GPT_Label_Regex': label\n+        })\n+    return results\n+\n+def compile_category_patterns(merchant_gpt, type_gpt):\n+    \"\"\"\n+    Compile a dictionary of regex: category from GPT results,\n+    skipping 'Other' and generic patterns.\n+    \"\"\"\n+    patterns = {}\n+    for row in merchant_gpt + type_gpt:\n+        suggestion = row['GPT_Label_Regex']\n+        if '|' in suggestion:\n+            cat, regex = [s.strip() for s in suggestion.split('|', 1)]\n+            if cat and regex and cat != \"Other\" and regex != \".*\":\n+                patterns[regex] = cat\n+    return patterns\n+\n+def analyze_and_gpt(csv_path, top_n=DEFAULT_TOP_N):\n+    \"\"\"\n+    Main analysis function:\n+    - Loads CSV\n+    - Extracts merchant/type clues\n+    - Saves top clues\n def analyze_and_gpt(csv_path, top_n=30):\n     # Load and filter\n     df = pd.read_csv(csv_path)\n     df['Post Date'] = pd.to_datetime(df['Post Date'], errors='coerce')\n"
                },
                {
                    "date": 1749264888609,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -134,8 +134,12 @@\n     Main analysis function:\n     - Loads CSV\n     - Extracts merchant/type clues\n     - Saves top clues\n+    - Labels clues with GPT (with caching)\n+    - Saves labeled clues\n+    - Prints ready-to-paste CATEGORY_PATTERNS\n+    \"\"\"\n def analyze_and_gpt(csv_path, top_n=30):\n     # Load and filter\n     df = pd.read_csv(csv_path)\n     df['Post Date'] = pd.to_datetime(df['Post Date'], errors='coerce')\n"
                },
                {
                    "date": 1749264894511,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -138,8 +138,14 @@\n     - Labels clues with GPT (with caching)\n     - Saves labeled clues\n     - Prints ready-to-paste CATEGORY_PATTERNS\n     \"\"\"\n+    # Load and filter data\n+    try:\n+        df = pd.read_csv(csv_path)\n+    except Exception as e:\n+        logging.error(f\"Failed to read CSV: {e}\")\n+        sys.exit(1)\n def analyze_and_gpt(csv_path, top_n=30):\n     # Load and filter\n     df = pd.read_csv(csv_path)\n     df['Post Date'] = pd.to_datetime(df['Post Date'], errors='coerce')\n"
                },
                {
                    "date": 1749264907914,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -144,62 +144,60 @@\n         df = pd.read_csv(csv_path)\n     except Exception as e:\n         logging.error(f\"Failed to read CSV: {e}\")\n         sys.exit(1)\n-def analyze_and_gpt(csv_path, top_n=30):\n-    # Load and filter\n-    df = pd.read_csv(csv_path)\n+    if 'Post Date' not in df.columns:\n+        logging.error(\"CSV must have a 'Post Date' column!\")\n+        sys.exit(1)\n     df['Post Date'] = pd.to_datetime(df['Post Date'], errors='coerce')\n     df = df[df['Post Date'] >= '2023-01-01']\n \n-    # Top clues\n-    df['Merchant'] = df['Description'].str.extract(r'(?:CO:|NAME:)\\s*([A-Za-z0-9\\.\\- ]+)')\n-    df['TypeAfter'] = df['Description'].str.extract(r'TYPE:\\s*([A-Za-z0-9\\.\\- ]+)')\n+    # Extract clues\n+    merchant, type_after = extract_clues(df)\n \n-    merchant_counts = df['Merchant'].value_counts().dropna().head(top_n)\n-    type_counts = df['TypeAfter'].value_counts().dropna().head(top_n)\n+    # Save top clues\n+    merchant_counts = save_counts(merchant, MERCHANT_CLUES_CSV, top_n)\n+    type_counts = save_counts(type_after, TYPE_CLUES_CSV, top_n)\n+    logging.info(f\"Saved top {top_n} merchant clues to {MERCHANT_CLUES_CSV}\")\n+    logging.info(f\"Saved top {top_n} type clues to {TYPE_CLUES_CSV}\")\n \n-    # Save summaries\n-    merchant_counts.to_csv(\"merchant_clues_auto.csv\")\n-    type_counts.to_csv(\"type_clues_auto.csv\")\n+    # Load GPT cache\n+    gpt_cache = load_gpt_cache(GPT_CACHE_FILE)\n \n-    # --- GPT-4o Labeling\n-    try:\n-        with open(GPT_CACHE_FILE, \"r\") as f:\n-            gpt_cache = json.load(f)\n-    except FileNotFoundError:\n-        gpt_cache = {}\n+    # Label clues with GPT\n+    logging.info(\"Labeling merchant clues with GPT...\")\n+    merchant_gpt = label_clues_with_gpt(merchant_counts, 'Merchant_Clue', gpt_cache)\n+    logging.info(\"Labeling type clues with GPT...\")\n+    type_gpt = label_clues_with_gpt(type_counts, 'Type_Clue', gpt_cache)\n \n-    merchant_gpt = []\n-    for merchant in merchant_counts.index:\n-        label = gpt_label_pattern(merchant, gpt_cache)\n-        merchant_gpt.append({'Merchant_Clue': merchant, 'Count': int(merchant_counts[merchant]), 'GPT_Label_Regex': label})\n+    # Save labeled clues\n+    pd.DataFrame(merchant_gpt).to_csv(MERCHANT_GPT_CSV, index=False)\n+    pd.DataFrame(type_gpt).to_csv(TYPE_GPT_CSV, index=False)\n+    logging.info(f\"Saved GPT-labeled merchant clues to {MERCHANT_GPT_CSV}\")\n+    logging.info(f\"Saved GPT-labeled type clues to {TYPE_GPT_CSV}\")\n \n-    type_gpt = []\n-    for type_c in type_counts.index:\n-        label = gpt_label_pattern(type_c, gpt_cache)\n-        type_gpt.append({'Type_Clue': type_c, 'Count': int(type_counts[type_c]), 'GPT_Label_Regex': label})\n+    # Save updated GPT cache\n+    save_gpt_cache(gpt_cache, GPT_CACHE_FILE)\n \n-    pd.DataFrame(merchant_gpt).to_csv(\"merchant_clues_gpt_auto.csv\", index=False)\n-    pd.DataFrame(type_gpt).to_csv(\"type_clues_gpt_auto.csv\", index=False)\n-    with open(GPT_CACHE_FILE, \"w\") as f:\n-        json.dump(gpt_cache, f, indent=2)\n-\n-    # Compile CATEGORY_PATTERNS\n-    patterns = {}\n-    for row in merchant_gpt + type_gpt:\n-        suggestion = row['GPT_Label_Regex']\n-        if '|' in suggestion:\n-            cat, regex = [s.strip() for s in suggestion.split('|', 1)]\n-            if cat != \"Other\" and regex and regex != \".*\":\n-                patterns[regex] = cat\n+    # Compile and print CATEGORY_PATTERNS\n+    patterns = compile_category_patterns(merchant_gpt, type_gpt)\n     print(\"\\n--- Ready-to-paste CATEGORY_PATTERNS ---\\n\")\n     print(\"CATEGORY_PATTERNS = {\")\n     for regex, cat in patterns.items():\n         print(f'    r\"{regex}\": \"{cat}\",')\n     print(\"}\")\n-\n     print(\"\\nMerchant and type pattern CSVs are saved for further review.\")\n \n+def main():\n+    setup_openai()\n+    csv_path = input(\"Enter path to your AccountHistory.csv: \").strip()\n+    if not csv_path or not Path(csv_path).exists():\n+        logging.error(f\"File not found: {csv_path}\")\n+        sys.exit(1)\n+    try:\n+        top_n = int(input(f\"How many top clues to analyze? (default {DEFAULT_TOP_N}): \").strip() or DEFAULT_TOP_N)\n+    except Exception:\n+        top_n = DEFAULT_TOP_N\n+    analyze_and_gpt(csv_path, top_n=top_n)\n+\n if __name__ == \"__main__\":\n-    csv_path = input(\"Enter path to your AccountHistory.csv: \").strip()\n-    analyze_and_gpt(csv_path, top_n=30)\n+    main()\n"
                }
            ],
            "date": 1749264806079,
            "name": "Commit-0",
            "content": "import os, re, json\nimport pandas as pd\nfrom collections import Counter\nfrom dotenv import load_dotenv\n\nimport openai\nload_dotenv(os.path.expanduser(\"~/.env\"))\nopenai.api_key = os.getenv(\"OPENAI_API_KEY\")\nGPT_CACHE_FILE = \"gpt_pattern_suggestions_auto.json\"\n\ndef gpt_label_pattern(pattern_desc, cache):\n    if pattern_desc in cache:\n        return cache[pattern_desc]\n    prompt = (\n        f\"This is a US bank transaction merchant clue or substring: '{pattern_desc}'. \"\n        \"Classify it with a finance category (Rent, Utilities, Streaming, Shopping, Bank Fee, SaaS, Payment, Food, Gas, Travel, Other) \"\n        \"AND give a regex pattern to match all similar descriptions. \"\n        \"Format: CATEGORY | REGEX\"\n    )\n    try:\n        resp = openai.ChatCompletion.create(\n            model=\"gpt-4o\",\n            messages=[\n                {\"role\": \"system\", \"content\": \"You are a financial data wrangler and regex expert.\"},\n                {\"role\": \"user\", \"content\": prompt}\n            ],\n            max_tokens=32,\n            temperature=0.1,\n        )\n        reply = resp['choices'][0]['message']['content'].strip()\n    except Exception as e:\n        print(f\"GPT ERROR: {e} for {pattern_desc[:30]}\")\n        reply = \"Other | .*\"\n    cache[pattern_desc] = reply\n    return reply\n\ndef analyze_and_gpt(csv_path, top_n=30):\n    # Load and filter\n    df = pd.read_csv(csv_path)\n    df['Post Date'] = pd.to_datetime(df['Post Date'], errors='coerce')\n    df = df[df['Post Date'] >= '2023-01-01']\n\n    # Top clues\n    df['Merchant'] = df['Description'].str.extract(r'(?:CO:|NAME:)\\s*([A-Za-z0-9\\.\\- ]+)')\n    df['TypeAfter'] = df['Description'].str.extract(r'TYPE:\\s*([A-Za-z0-9\\.\\- ]+)')\n\n    merchant_counts = df['Merchant'].value_counts().dropna().head(top_n)\n    type_counts = df['TypeAfter'].value_counts().dropna().head(top_n)\n\n    # Save summaries\n    merchant_counts.to_csv(\"merchant_clues_auto.csv\")\n    type_counts.to_csv(\"type_clues_auto.csv\")\n\n    # --- GPT-4o Labeling\n    try:\n        with open(GPT_CACHE_FILE, \"r\") as f:\n            gpt_cache = json.load(f)\n    except FileNotFoundError:\n        gpt_cache = {}\n\n    merchant_gpt = []\n    for merchant in merchant_counts.index:\n        label = gpt_label_pattern(merchant, gpt_cache)\n        merchant_gpt.append({'Merchant_Clue': merchant, 'Count': int(merchant_counts[merchant]), 'GPT_Label_Regex': label})\n\n    type_gpt = []\n    for type_c in type_counts.index:\n        label = gpt_label_pattern(type_c, gpt_cache)\n        type_gpt.append({'Type_Clue': type_c, 'Count': int(type_counts[type_c]), 'GPT_Label_Regex': label})\n\n    pd.DataFrame(merchant_gpt).to_csv(\"merchant_clues_gpt_auto.csv\", index=False)\n    pd.DataFrame(type_gpt).to_csv(\"type_clues_gpt_auto.csv\", index=False)\n    with open(GPT_CACHE_FILE, \"w\") as f:\n        json.dump(gpt_cache, f, indent=2)\n\n    # Compile CATEGORY_PATTERNS\n    patterns = {}\n    for row in merchant_gpt + type_gpt:\n        suggestion = row['GPT_Label_Regex']\n        if '|' in suggestion:\n            cat, regex = [s.strip() for s in suggestion.split('|', 1)]\n            if cat != \"Other\" and regex and regex != \".*\":\n                patterns[regex] = cat\n    print(\"\\n--- Ready-to-paste CATEGORY_PATTERNS ---\\n\")\n    print(\"CATEGORY_PATTERNS = {\")\n    for regex, cat in patterns.items():\n        print(f'    r\"{regex}\": \"{cat}\",')\n    print(\"}\")\n\n    print(\"\\nMerchant and type pattern CSVs are saved for further review.\")\n\nif __name__ == \"__main__\":\n    csv_path = input(\"Enter path to your AccountHistory.csv: \").strip()\n    analyze_and_gpt(csv_path, top_n=30)\n"
        }
    ]
}